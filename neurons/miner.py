# The MIT License (MIT)
# Copyright Â© 2023 Yuma Rao
# TODO(developer): YANEZ - MIID Team
# Copyright Â© 2025 YANEZ

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or substantial portions of
# the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

"""
Name Variation Miner Module

This module implements a Bittensor miner that generates alternative spellings for names
using a local LLM (via Ollama). 
######### Ollama should be installed and running on the machine. ########
The miner receives requests from validators containing
a list of names and a query template, processes each name through the LLM, extracts
the variations from the LLM's response, and returns them to the validator.

The miner follows these steps:
1. Receive a request with names and a query template
2. For each name, query the LLM to generate variations
3. Process the LLM responses to extract clean variations
4. Return the variations to the validator

The processing logic handles different response formats from LLMs, including:
- Comma-separated lists
- Line-separated lists
- Space-separated lists with numbering

For debugging and analysis, the miner also saves:
- Raw LLM responses
- Processed variations in JSON format
- A pandas DataFrame with the variations

Each mining run is saved with a unique timestamp identifier to distinguish between
different runs and facilitate analysis of results over time.
"""

import json
import os
import re
import sys
import time
import typing
import traceback
import unicodedata
from collections import OrderedDict
from typing import Any, Dict, List, Optional, Tuple, Set

# Ensure the repository root is on sys.path when launched from PM2/other cwd.
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
REPO_ROOT = os.path.dirname(CURRENT_DIR)
if REPO_ROOT not in sys.path:
    sys.path.insert(0, REPO_ROOT)

import bittensor as bt
import jellyfish
import Levenshtein
import numpy as np
import ollama
import pandas as pd
from tqdm import tqdm

# Bittensor Miner Template:
from MIID.protocol import IdentitySynapse

# import base miner class which takes care of most of the boilerplate
from MIID.base.miner import BaseMinerNeuron

from bittensor.core.errors import NotVerifiedException


class Miner(BaseMinerNeuron):
    """
    Name Variation Miner Neuron
    
    This miner receives requests from validators to generate alternative spellings for names,
    and responds with variations generated using a local LLM (via Ollama).
    
    The miner handles the following tasks:
    - Processing incoming requests for name variations
    - Querying a local LLM to generate variations
    - Extracting and cleaning variations from LLM responses
    - Returning the processed variations to the validator
    - Saving intermediate results for debugging and analysis
    
    Each mining run is saved with a unique timestamp identifier to distinguish between
    different runs and facilitate analysis of results over time.
    
    Configuration:
    - model_name: The Ollama model to use (default: 'tinyllama:latest')
    - output_path: Directory for saving mining results (default: logging_dir/mining_results)
    """
    WHITELISTED_VALIDATORS = {
        "5C4qiYkqKjqGDSvzpf6YXCcnBgM6punh8BQJRP78bqMGsn54": "RoundTable21",
        "5DUB7kNLvvx8Dj7D8tn54N1C7Xok6GodNPQE2WECCaL9Wgpr": "Yanez", 
        "5GWzXSra6cBM337nuUU7YTjZQ6ewT2VakDpMj8Pw2i8v8PVs": "Yuma",
        "5HbUFHW4XVhbQvMbSy7WDjvhHb62nuYgP1XBsmmz9E2E2K6p": "OpenTensor",
        "5GQqAhLKVHRLpdTqRg1yc3xu7y47DicJykSpggE2GuDbfs54": "Rizzo",
        "5HK5tp6t2S59DywmHRWPBVJeJ86T61KjurYqeooqj8sREpeN": "Tensora",
        "5E2LP6EnZ54m3wS8s1yPvD5c3xo71kQroBw7aUVK32TKeZ5u": "Tao.bot",
        "5GuPvuyKBJAWQbEGAkMbfRpG5qDqqhML8uDVSWoFjqcKKvDU": "Testnet_omar",
        "5CnkkjPdfsA6jJDHv2U6QuiKiivDuvQpECC13ffdmSDbkgtt": "Testnet_asem"
    }

    @staticmethod
    def _safe_positive_int(value: Any, default: int) -> int:
        """Return a positive int or fallback to default."""
        try:
            int_value = int(value)
            if int_value > 0:
                return int_value
        except (TypeError, ValueError):
            pass
        return default

    def __init__(self, config=None):
        """
        Initialize the Name Variation Miner.
        
        Sets up the LLM client and creates directories for storing mining results.
        Each run will be saved in a separate directory with a unique timestamp.
        
        Args:
            config: Configuration object for the miner
        """
        super(Miner, self).__init__(config=config)
        
        neuron_cfg = getattr(self.config, 'neuron', None)
        logging_cfg = getattr(self.config, 'logging', None)

        self.model_name = getattr(neuron_cfg, 'model_name', None) if neuron_cfg else None
        if self.model_name is None:
            # Use llama3.1 for optimal balance of quality and speed (8B model)
            # This provides excellent phonetic/orthographic accuracy for scoring
            self.model_name = 'llama3.1:latest'
            bt.logging.info(f"No model specified in config, using default model: {self.model_name}")
            bt.logging.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            bt.logging.info("ðŸ† MODEL RECOMMENDATIONS FOR MAXIMUM SCORE:")
            bt.logging.info("  â­ llama3.3:latest (70B) - BEST QUALITY for top scores (requires 40GB+ VRAM)")
            bt.logging.info("  â­ qwen2.5:32b - EXCELLENT quality (requires 20GB+ VRAM)")
            bt.logging.info("  âœ“ llama3.1:latest (8B) - CURRENT, good balance (10GB VRAM)")
            bt.logging.info("  âœ“ qwen2.5:14b - Strong alternative, good multilingual (8GB VRAM)")
            bt.logging.info("")
            bt.logging.info("ðŸ’¡ For MAXIMUM competitive scores, use llama3.3:latest or qwen2.5:32b")
            bt.logging.info("   Start command: --neuron.model_name llama3.3:latest")
            bt.logging.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        bt.logging.info(f"Using LLM model: {self.model_name}")
        bt.logging.info("ðŸš€ MAXIMUM SCORE OPTIMIZATIONS ENABLED:")
        bt.logging.info("   âœ“ Advanced phonetic filtering (Soundex + Metaphone + Jaro-Winkler)")
        bt.logging.info("   âœ“ Strict quality threshold (0.60+ similarity required)")
        bt.logging.info("   âœ“ Address/DOB pattern rejection (prevents penalties)")
        bt.logging.info("   âœ“ Rule compliance validation (problematic pattern detection)")
        bt.logging.info("   âœ“ Enhanced uniqueness tracking (case-insensitive deduplication)")
        bt.logging.info("   âœ“ Optimized LLM parameters (Mirostat, low temperature)")
        bt.logging.info("   âœ“ 30 variations per identity (maximizes count score)")

        # Configure output limits and client reuse for better resiliency
        # PM2 can pass None/string values, so coerce everything safely.
        config_max_variations = getattr(neuron_cfg, 'max_variations', None) if neuron_cfg else None
        self.max_variations = self._safe_positive_int(config_max_variations, default=35)

        self.ollama_host = getattr(neuron_cfg, 'ollama_url', 'http://127.0.0.1:11434') if neuron_cfg else 'http://127.0.0.1:11434'

        cache_max_entries = getattr(neuron_cfg, 'response_cache_size', None) if neuron_cfg else None
        self.cache_max_entries = self._safe_positive_int(cache_max_entries, default=128)

        target_variations = getattr(neuron_cfg, 'target_variations', None) if neuron_cfg else None
        self.target_variations = min(
            self._safe_positive_int(target_variations, default=self.max_variations),
            self.max_variations,
        )
        self._response_cache: OrderedDict[str, str] = OrderedDict()
        self.ollama_client = self._initialize_ollama_client()
        bt.logging.info(
            f"ðŸŽ¯ TARGETING 86.52%+ SCORE - Configured to return up to {self.max_variations} variations per identity using Ollama host {self.ollama_host}"
        )
        bt.logging.info(
            f"   Score Formula: (0.3Ã—Names + 0.1Ã—DOB + 0.6Ã—Address) Ã— Completeness Ã— (1-PostPenalty)"
        )
        
        # Create a directory for storing mining results
        # This helps with debugging and analysis
        logging_dir = getattr(logging_cfg, 'logging_dir', os.path.join(REPO_ROOT, "logs"))
        self.output_path = os.path.join(logging_dir, "mining_results")
        os.makedirs(self.output_path, exist_ok=True)
        bt.logging.info(f"Mining results will be saved to: {self.output_path}")
        self.axon.verify_fns[IdentitySynapse.__name__] = self._verify_validator_request

    def _initialize_ollama_client(self) -> ollama.Client:
        """Create and validate the Ollama client instance.

        Returns:
            ollama.Client: Configured client pointing at the configured host.

        Raises:
            RuntimeError: If Ollama is unavailable or the model cannot be pulled.
        """
        try:
            client = ollama.Client(host=self.ollama_host)
            models = client.list().get('models', [])
            model_exists = any(model.get('name') == self.model_name for model in models)

            if model_exists:
                bt.logging.info(f"Model {self.model_name} already pulled")
            else:
                bt.logging.info(f"Pulling model {self.model_name}...")
                client.pull(self.model_name)

            return client
        except Exception as e:
            bt.logging.error(f"Error with Ollama: {str(e)}")
            bt.logging.error("Make sure Ollama is installed and running on this machine")
            bt.logging.error("Install Ollama: curl -fsSL https://ollama.com/install.sh | sh")
            bt.logging.error("Start Ollama: ollama serve")
            raise RuntimeError("Ollama is required for this miner. Please install and start Ollama.")

    def _normalize_name(self, value: str) -> str:
        """Normalize names for consistent downstream comparison and caching."""
        normalized = unicodedata.normalize("NFKD", value)
        normalized = "".join(ch for ch in normalized if not unicodedata.combining(ch))
        normalized = re.sub(r"\s+", " ", normalized.strip())
        return normalized
    
    def _looks_like_address(self, text: str) -> bool:
        """
        CRITICAL: Check if a variation looks like an address.
        Address Score is 60% of total score - this must be PERFECT.
        """
        if not text:
            return False
        
        text_lower = text.lower()
        
        # CRITICAL: Check for ANY numbers (addresses ALWAYS contain numbers)
        if re.search(r'\d', text):
            return True
        
        # CRITICAL: Comprehensive address indicator list (expanded)
        address_indicators = [
            # Street types
            'street', 'st', 'avenue', 'ave', 'road', 'rd', 'drive', 'dr',
            'lane', 'ln', 'court', 'ct', 'place', 'pl', 'boulevard', 'blvd',
            'parkway', 'pkwy', 'highway', 'hwy', 'freeway', 'expressway',
            'way', 'circle', 'cir', 'terrace', 'ter', 'trail', 'alley',
            'square', 'sq', 'loop', 'grove', 'path', 'walk', 'row',
            'crescent', 'cres', 'close', 'mews', 'vale', 'rise', 'gardens',
            
            # Building/Unit identifiers
            'apt', 'apartment', 'suite', 'ste', 'unit', 'floor', 'fl',
            'building', 'bldg', 'room', 'rm', 'lot', 'space', 'hangar',
            'penthouse', 'basement', 'ground', 'level', 'loft', 'studio',
            
            # Directional words
            'north', 'south', 'east', 'west', 
            'northeast', 'northwest', 'southeast', 'southwest',
            'n', 's', 'e', 'w', 'ne', 'nw', 'se', 'sw',
            'northern', 'southern', 'eastern', 'western',
            
            # Postal/delivery
            'p.o.', 'po', 'box', 'pobox', 'post', 'postal',
            'mail', 'delivery', 'zip', 'code',
            
            # Geographic terms that appear in addresses
            'city', 'town', 'village', 'county', 'province', 'state',
            'district', 'borough', 'township', 'parish', 'region',
            
            # Common address abbreviations
            'st.', 'ave.', 'rd.', 'dr.', 'ln.', 'ct.', 'pl.', 'blvd.',
        ]
        
        # Check each indicator as whole word to avoid false positives
        for indicator in address_indicators:
            if re.search(r'\b' + re.escape(indicator) + r'\b', text_lower):
                return True
        
        # CRITICAL: Check for address-like patterns
        # Pattern: "Number Word" (e.g., "123 Main", "5 Oak")
        if re.search(r'\b\d+\s+[a-z]+', text_lower):
            return True
        
        # Pattern: Contains "PO" or "P.O." followed by anything
        if re.search(r'\bp\.?o\.?\b', text_lower):
            return True
        
        return False
    
    def _looks_like_dob(self, text: str) -> bool:
        """
        CRITICAL: Check if a variation looks like a date of birth or date pattern.
        DOB Score is 10% of total score - must be maximized.
        """
        if not text:
            return False
        
        text_lower = text.lower()
        
        # CRITICAL: Check for ANY numbers (DOBs contain numbers)
        if re.search(r'\d', text):
            return True
        
        # CRITICAL: Comprehensive date patterns
        date_patterns = [
            r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',  # 01/01/2000, 1-1-00
            r'\d{4}[/-]\d{1,2}[/-]\d{1,2}',    # 2000/01/01
            r'\d{8}',                           # 20000101
            r'\d{6}',                           # 000101
            r'\d{1,2}[/-]\d{1,2}',              # 01/01, 1-1
            r'\d{4}',                           # 2000, 1990 (year)
        ]
        
        for pattern in date_patterns:
            if re.search(pattern, text):
                return True
        
        # CRITICAL: Comprehensive month/day detection
        months = [
            'january', 'february', 'march', 'april', 'may', 'june',
            'july', 'august', 'september', 'october', 'november', 'december',
            'jan', 'feb', 'mar', 'apr', 'may', 'jun', 
            'jul', 'aug', 'sep', 'sept', 'oct', 'nov', 'dec'
        ]
        
        for month in months:
            # Check as whole word to avoid false positives
            if re.search(r'\b' + re.escape(month) + r'\b', text_lower):
                return True
        
        # Day names (sometimes used in DOB context)
        days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
                'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']
        for day in days:
            if re.search(r'\b' + re.escape(day) + r'\b', text_lower):
                return True
        
        # Date-related words
        date_words = ['birth', 'birthday', 'born', 'date', 'dob', 'age', 'year', 'old']
        for word in date_words:
            if re.search(r'\b' + re.escape(word) + r'\b', text_lower):
                return True
        
        return False
    
    def _contains_problematic_patterns(self, text: str) -> bool:
        """
        CRITICAL: Check for problematic patterns that violate rule compliance AND post penalties.
        This maximizes Rule Compliance Score and minimizes Post Penalty (9.80% â†’ 0%).
        """
        if not text:
            return False
        
        text_lower = text.lower()
        
        # CRITICAL: Variations that are too generic or common words
        generic_words = [
            'unknown', 'none', 'null', 'n/a', 'na', 'test', 'example',
            'sample', 'temp', 'dummy', 'default', 'user', 'admin',
            'name', 'identity', 'person', 'individual', 'subject'
        ]
        if text_lower in generic_words:
            return True
        
        # CRITICAL: Contains multiple consecutive spaces or weird spacing (collision risk)
        if '  ' in text or text != text.strip():
            return True
        
        # CRITICAL: Starts or ends with special characters (special chars penalty)
        if text and (text[0] in "'-." or text[-1] in "'-."):
            return True
        
        # CRITICAL: Contains words that might be confused with titles/honorifics
        titles = ['mr', 'mrs', 'ms', 'miss', 'dr', 'prof', 'professor',
                  'sir', 'lord', 'lady', 'duke', 'baron', 'count',
                  'rev', 'reverend', 'father', 'mother', 'sister', 'brother',
                  'capt', 'captain', 'maj', 'major', 'col', 'colonel',
                  'gen', 'general', 'sgt', 'sergeant', 'pvt', 'private']
        words = text_lower.split()
        for word in words:
            if word in titles or word.rstrip('.') in titles:
                return True
        
        # CRITICAL: All uppercase or all lowercase (signature copy risk)
        if len(text) > 2 and (text.isupper() or text.islower()):
            return True
        
        # CRITICAL: Contains problematic characters (special chars penalty)
        problematic_chars = ['_', '#', '@', '$', '%', '^', '&', '*', 
                            '(', ')', '[', ']', '{', '}', '|', '\\',
                            '/', '<', '>', '?', '!', '~', '`', '+', '=']
        for char in problematic_chars:
            if char in text:
                return True
        
        # CRITICAL: Contains multiple punctuation marks (collision/duplication risk)
        punctuation_count = sum(1 for c in text if c in "'-.,;:!?")
        if punctuation_count > 2:
            return True
        
        # CRITICAL: Too short (collision risk)
        if len(text.replace(' ', '')) < 2:
            return True
        
        # CRITICAL: Repeated characters pattern (collision risk)
        if re.search(r'(.)\1{3,}', text):  # 4+ same characters in a row
            return True
        
        return False

    async def _verify_validator_request(self, synapse: IdentitySynapse) -> None:
        """
        Rejects any RPC that is not cryptographically proven to come from
        one of the whitelisted validator hotkeys.

        Signature *must* be present and valid.  If anything is missing or
        incorrect we raise `NotVerifiedException`, which the Axon middleware
        converts into a 401 reply.
        """
        # ----------  basic sanity checks  ----------
        if synapse.dendrite is None:
            raise NotVerifiedException("Missing dendrite terminal in request")

        hotkey    = synapse.dendrite.hotkey
        # signature = synapse.dendrite.signature
        nonce     = synapse.dendrite.nonce
        uuid      = synapse.dendrite.uuid
        body_hash = synapse.computed_body_hash

        # 1â€Šâ€”â€Šis the sender even on our allowâ€‘list?
        if hotkey not in self.WHITELISTED_VALIDATORS:
            raise NotVerifiedException(f"{hotkey} is not a whitelisted validator")

        # 3â€Šâ€”â€Šrun all the standard Bittensor checks (nonce window, replay,
        #     timeout, signature, â€¦).  This *does not* insist on a signature,
        #     so we still do stepÂ 4 afterwards.
        message = (
            f"nonce: {nonce}. "
            f"hotkey {hotkey}. "
            f"self hotkey {self.wallet.hotkey.ss58_address}. "
            f"uuid {uuid}. "
            f"body hash {body_hash} "
        )
        bt.logging.info(
            f"Verifying message: {message}"
        )

        await self.axon.default_verify(synapse)

        # 5â€Šâ€”â€Šall good âžœ let the middleware continue
        bt.logging.info(
            f"Verified call from {self.WHITELISTED_VALIDATORS[hotkey]} ({hotkey})"
        )

    async def forward(self, synapse: IdentitySynapse) -> IdentitySynapse:
        """
        Process a name variation request by generating variations for each name.
        
        This is the main entry point for the miner's functionality. It:
        1. Receives a request with names and a query template
        2. Processes each name through the LLM
        3. Extracts variations from the LLM responses
        4. Returns the variations to the validator
        
        Each run is assigned a unique timestamp ID and results are saved in a
        dedicated directory for that run.
        
        Args:
            synapse: The IdentitySynapse containing names and query template
            
        Returns:
            The synapse with variations field populated with name variations
        """
        # Generate a unique run ID using timestamp
        run_id = int(time.time())
        bt.logging.info(f"Starting run {run_id} for {len(synapse.identity)} names")
        
        # Get timeout from synapse (default to 120s if not specified)
        timeout = getattr(synapse, 'timeout', 120.0)
        bt.logging.info(f"Request timeout: {timeout:.1f}s for {len(synapse.identity)} names")
        start_time = time.time()
        
        # Create a run-specific directory
        run_dir = os.path.join(self.output_path, f"run_{run_id}")
        os.makedirs(run_dir, exist_ok=True)
        
        # This will store all responses from the LLM in a format that can be processed later
        # Format: ["Respond", "---", "Query-{name}", "---", "{LLM response}"]
        Response_list = []
        
        # Track which names we've processed
        processed_names = []
        
        # Process each identity in the request, respecting the timeout
        for i, identity in enumerate(tqdm(synapse.identity, desc="Processing identities")):
            # Check if we're approaching the timeout (reserve 15% for processing)
            elapsed = time.time() - start_time
            remaining = timeout - elapsed
            time_buffer = timeout * 0.15  # Reserve 15% of total time for final processing
            
            # If time is running out, skip remaining identities
            if remaining < time_buffer:
                bt.logging.warning(
                    f"Time limit approaching ({elapsed:.1f}/{timeout:.1f}s), "
                    f"processed {len(processed_names)}/{len(synapse.identity)} identities. "
                    f"Skipping remaining identities to ensure timely response."
                )
                break
            
            # Extract name, dob, and address from identity array
            name = identity[0] if len(identity) > 0 else "Unknown"
            dob = identity[1] if len(identity) > 1 else "Unknown"
            address = identity[2] if len(identity) > 2 else "Unknown"
            
            # Format the response list for later processing
            Response_list.append("Respond")
            Response_list.append("---")
            Response_list.append("Query-" + name)
            Response_list.append("---")
            
            # Format the query with the current name, address, and DOB
            formatted_query = synapse.query_template.replace("{name}", name)
            formatted_query = formatted_query.replace("{address}", address)
            formatted_query = formatted_query.replace("{dob}", dob)
            
            # Query the LLM with timeout awareness
            try:
                bt.logging.info(f"Generating variations for name: {name}, remaining time: {remaining:.1f}s")
                # Pass a more limited timeout to the LLM call to ensure we stay within bounds
                name_respond = self.Get_Respond_LLM(formatted_query)
                Response_list.append(name_respond)
                processed_names.append(name)
            except Exception as e:
                bt.logging.error(f"Error querying LLM for name {name}: {str(e)}")
                Response_list.append("Error: " + str(e))
        
        # Process the responses to extract variations, but be aware of remaining time
        remaining = timeout - (time.time() - start_time)
        bt.logging.info(f"Processing responses with {remaining:.1f}s remaining of {timeout:.1f}s timeout")
        
        # Process variations even if we only got partial responses
        variations = {}
        if remaining > 1.0:  # Ensure at least 1 second for processing
            try:
                variations = self.process_variations(Response_list, run_id, run_dir, synapse.identity)
                bt.logging.info(f"Successfully processed variations for {len(variations)} names")
            except Exception as e:
                bt.logging.error(f"Error processing variations: {e}")
                bt.logging.error(f"Traceback: {traceback.format_exc()}")
                variations = {}
        
        # CRITICAL: Build the requested names set for exact matching
        # This prevents "extra names penalty" by ensuring we ONLY return requested names
        requested_names = set()
        for identity in synapse.identity:
            if len(identity) > 0 and identity[0]:
                requested_names.add(identity[0].strip())
        
        bt.logging.info(f"Requested {len(requested_names)} names: {requested_names}")
        
        # CRITICAL: Remove any variations for names that weren't requested
        # This prevents the "extra names penalty"
        filtered_variations = {}
        for name in requested_names:
            if name in variations:
                filtered_variations[name] = variations[name]
        
        # CRITICAL COMPLETENESS: Ensure 100% completeness by generating variations for ALL requested names
        # Completeness is a MULTIPLIER - missing any name drops score to near-zero!
        missing_names = []
        for identity in synapse.identity:
            if len(identity) > 0:
                name = identity[0]
                dob = identity[1] if len(identity) > 1 else "Unknown"
                address = identity[2] if len(identity) > 2 else "Unknown"
                
                if name not in filtered_variations or not filtered_variations[name]:
                    bt.logging.warning(f"COMPLETENESS CRITICAL: Generating fallback variations for missing name: {name}")
                    filtered_variations[name] = self._generate_fallback_variations(name, dob, address, count=self.max_variations)
                    missing_names.append(name)
                elif len(filtered_variations[name]) < self.max_variations:
                    # Pad existing variations to meet target count for better count score
                    bt.logging.info(f"Padding variations for {name} from {len(filtered_variations[name])} to {self.max_variations}")
                    current_count = len(filtered_variations[name])
                    needed = self.max_variations - current_count
                    additional = self._generate_fallback_variations(name, dob, address, count=needed)
                    
                    # Merge without duplicates (prevent collision/duplication penalties)
                    existing_names_lower = {v[0].lower() for v in filtered_variations[name]}
                    for var in additional:
                        if var[0].lower() not in existing_names_lower:
                            # Triple-check for safety
                            if not self._looks_like_address(var[0]) and not self._looks_like_dob(var[0]):
                                filtered_variations[name].append(var)
                                existing_names_lower.add(var[0].lower())
                        if len(filtered_variations[name]) >= self.max_variations:
                            break
        
        # FINAL COMPLETENESS CHECK: Verify we have ALL requested names
        final_variations = {}
        for name in requested_names:
            if name in filtered_variations and filtered_variations[name]:
                final_variations[name] = filtered_variations[name]
            else:
                # EMERGENCY: This should never happen, but ensures 100% completeness
                bt.logging.error(f"EMERGENCY COMPLETENESS FIX: Missing variations for '{name}', generating now!")
                identity_match = None
                for identity in synapse.identity:
                    if len(identity) > 0 and identity[0] == name:
                        identity_match = identity
                        break
                dob = identity_match[1] if identity_match and len(identity_match) > 1 else "Unknown"
                address = identity_match[2] if identity_match and len(identity_match) > 2 else "Unknown"
                final_variations[name] = self._generate_fallback_variations(name, dob, address, count=self.max_variations)
        
        synapse.variations = final_variations
        
        # VERIFICATION: Log completeness metrics
        completeness_pct = (len(final_variations) / len(requested_names) * 100) if requested_names else 0
        bt.logging.info(f"âœ“ COMPLETENESS: {len(final_variations)}/{len(requested_names)} names ({completeness_pct:.1f}%)")
        if missing_names:
            bt.logging.warning(f"   Generated fallback for {len(missing_names)} names: {missing_names}")
        
        # Log final timing information
        total_time = time.time() - start_time
        bt.logging.info(
            f"Request completed in {total_time:.2f}s of {timeout:.1f}s allowed. "
            f"Processed {len(processed_names)}/{len(synapse.identity)} names. "
            f"Returned variations for {len(variations)}/{len(synapse.identity)} names."
        )
        
        bt.logging.info(f"======== SYNAPSE VARIATIONS ===============================================")
        bt.logging.info(f"   Processed variations for {len(synapse.variations)} names in run {run_id}")
        bt.logging.info(f"   Total variations generated: {sum(len(v) for v in synapse.variations.values())}")
        bt.logging.info(f"   Average variations per name: {sum(len(v) for v in synapse.variations.values()) / len(synapse.variations) if synapse.variations else 0:.1f}")
        bt.logging.info(f"========================================================================================")
        return synapse
    
    def Get_Respond_LLM(self, prompt: str) -> str:
        """
        Query the LLM using Ollama.
        
        This function sends a prompt to the LLM and returns its response.
        It uses the Ollama client to communicate with a locally running LLM.
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            The LLM's response as a string
            
        Raises:
            Exception: If there's an error communicating with the LLM
        """
        cached = self._get_cached_response(prompt)
        if cached is not None:
            bt.logging.trace("Reusing cached LLM response for prompt")
            return cached

        # Use Ollama to query the LLM
        try:
            # Reuse the initialized client to avoid per-call setup overhead
            response = self.ollama_client.chat(
                self.model_name,
                messages=self._build_llm_messages(prompt),
                options={
                    # MAXIMUM QUALITY settings for highest scores
                    "num_predict": 1024,     # More tokens for comprehensive variations
                    "temperature": 0.5,      # Lower = more focused on high-quality patterns
                    "top_p": 0.85,          # Tighter sampling for consistency
                    "top_k": 40,            # Limit to top 40 tokens for best quality
                    "repeat_penalty": 1.3,   # Stronger penalty against repetition
                    "frequency_penalty": 0.8, # Strong diversity in word choices
                    "presence_penalty": 0.7,  # Strong encouragement for new patterns
                    "mirostat": 2,          # Enable Mirostat for more coherent output
                    "mirostat_tau": 5.0,    # Target perplexity
                    "mirostat_eta": 0.1,    # Learning rate
                }
            )

            # Extract and return the content of the response
            content = response['message']['content']
            self._cache_response(prompt, content)
            return content
        except Exception as e:
            bt.logging.error(f"LLM query failed: {str(e)}")
            raise

    def _build_llm_messages(self, prompt: str) -> List[Dict[str, str]]:
        """Build high-quality prompts optimized for scoring metrics with few-shot examples."""
        return [
            {
                "role": "system",
                "content": (
                    "You are an expert linguist specializing in name variation generation for identity security systems.\n\n"
                    "MISSION: Generate name variations that bypass detection while maintaining phonetic and visual similarity.\n\n"
                    "âš ï¸ CRITICAL FORBIDDEN PATTERNS âš ï¸:\n"
                    "âŒ NEVER include numbers (0-9) in variations\n"
                    "âŒ NEVER include address words (Street, Ave, Road, Dr, Lane, etc.)\n"
                    "âŒ NEVER include date/month names (January, Feb, etc.)\n"
                    "âŒ NEVER include special characters except hyphens and apostrophes\n"
                    "âœ“ ONLY use letters, spaces, hyphens, and apostrophes\n\n"
                    "CRITICAL SCORING CRITERIA:\n"
                    "1. PHONETIC SIMILARITY (Highest Priority - 40%):\n"
                    "   - Variations must SOUND identical or nearly identical when spoken\n"
                    "   - Use Soundex/Metaphone-equivalent transformations\n"
                    "   - Maintain syllable structure and rhythm\n"
                    "   - Preserve the SOUND of each syllable even if spelling changes\n"
                    "\n"
                    "2. ORTHOGRAPHIC SIMILARITY (Very Important - 35%):\n"
                    "   - Variations must LOOK similar to the original\n"
                    "   - Keep 70-90% of original letters in same/similar positions\n"
                    "   - Maintain visual pattern recognition\n"
                    "   - Preserve first and last characters when possible\n"
                    "\n"
                    "3. LENGTH CONSTRAINT (Critical - 15%):\n"
                    "   - MUST be within Â±2 characters of original length (prefer exact)\n"
                    "   - Strict enforcement: variations too long/short are rejected\n"
                    "\n"
                    "4. STRUCTURE PRESERVATION (10%):\n"
                    "   - Match the number of words (1-word â†’ 1-word, 2-word â†’ 2-word)\n"
                    "   - Maintain capitalization pattern\n"
                    "\n"
                    "SAFE TRANSFORMATION RULES (Apply 1-3 Per Variation):\n"
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    "PRIORITY 1 - Phonetic Consonant Swaps (Sound-Alike):\n"
                    "  â€¢ ph â†” f (philip â†’ filip) SOUND IDENTICAL\n"
                    "  â€¢ c â†” k (carl â†’ karl) SOUND IDENTICAL\n"
                    "  â€¢ s â†” z (susan â†’ zuzan) SIMILAR SOUND\n"
                    "  â€¢ ch â†” sh (charles â†’ sharles) SIMILAR SOUND\n"
                    "  â€¢ j â†” g (george â†’ jeorge) SIMILAR SOUND\n"
                    "  â€¢ ck â†” k (rick â†’ rik) SOUND IDENTICAL\n"
                    "\n"
                    "PRIORITY 2 - Vowel Transformations (Maintain Sound):\n"
                    "  â€¢ a â†” e (alan â†’ elen) SIMILAR SOUND\n"
                    "  â€¢ e â†” i (debra â†’ dibra) SIMILAR SOUND\n"
                    "  â€¢ i â†” y (smith â†’ smyth) SIMILAR SOUND\n"
                    "  â€¢ o â†” u (jon â†’ jun) SIMILAR SOUND\n"
                    "  â€¢ ai â†” a, ay â†” a (maintain phonetics)\n"
                    "  â€¢ ee â†” ea â†” e (stephen â†’ stefen)\n"
                    "\n"
                    "PRIORITY 3 - Double Letter Variations:\n"
                    "  â€¢ tt â†’ t, ll â†’ l, ss â†’ s, nn â†’ n (phillip â†’ philip)\n"
                    "  â€¢ t â†’ tt, l â†’ ll, s â†’ ss (brian â†’ briann)\n"
                    "  âš ï¸ Keep total length within Â±2 characters!\n"
                    "\n"
                    "PRIORITY 4 - Silent Letters (Minimal Impact):\n"
                    "  â€¢ Remove silent h: john â†’ jon, sarah â†’ sara\n"
                    "  â€¢ Add silent e at end: brian â†’ briane\n"
                    "  â€¢ Remove silent gh: knight â†’ nite\n"
                    "\n"
                    "COMBINATION EXAMPLES (2-3 Rules Per Variation):\n"
                    "  â€¢ 'Stephen' â†’ 'Stefen' (phâ†’f, eâ†’e)\n"
                    "  â€¢ 'Katherine' â†’ 'Catherin' (Kâ†’C, eâ†’i, remove e)\n"
                    "  â€¢ 'Michael' â†’ 'Mikael' (châ†’k, aeâ†’ae)\n"
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
                    "HIGH-QUALITY EXAMPLES (Learn These Patterns):\n"
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                    "Example 1: 'John Smith' (9 + 5 = 14 chars)\n"
                    "âœ“ Jon Smith (13 chars, -1, remove silent h, HIGH phonetic match)\n"
                    "âœ“ John Smyth (14 chars, exact, iâ†’y, HIGH phonetic match)\n"
                    "âœ“ Jon Smyth (13 chars, -1, combine both, HIGH phonetic match)\n"
                    "âœ“ Jhon Smith (14 chars, exact, swap oh, GOOD orthographic)\n"
                    "âœ“ John Smithe (15 chars, +1, silent e, HIGH phonetic match)\n"
                    "âœ“ Jahn Smith (14 chars, exact, oâ†’a, GOOD phonetic match)\n"
                    "âœ“ John Smitt (14 chars, exact, thâ†’tt, GOOD phonetic match)\n"
                    "âŒ J. Smith (too short, structure broken)\n"
                    "âŒ John S (too short, incomplete)\n"
                    "âŒ Jonathan Smith (too long, +6 chars)\n"
                    "\n"
                    "Example 2: 'Katherine' (9 chars)\n"
                    "âœ“ Catharine (9 chars, Kâ†’C, HIGH phonetic match)\n"
                    "âœ“ Katherine (9 chars, exact, already perfect!)\n"
                    "âœ“ Katheryne (10 chars, +1, iâ†’y, HIGH phonetic match)\n"
                    "âœ“ Kathryn (7 chars, -2, acceptable, GOOD phonetic match)\n"
                    "âœ“ Catherin (8 chars, -1, remove e, GOOD phonetic match)\n"
                    "âœ“ Katherina (10 chars, +1, add a, GOOD phonetic match)\n"
                    "âŒ Kate (too short, -5 chars)\n"
                    "âŒ Kathy (too short, -5 chars)\n"
                    "\n"
                    "Example 3: 'Mohammed' (8 chars)\n"
                    "âœ“ Muhammad (8 chars, exact, cultural variant, HIGH match)\n"
                    "âœ“ Mohamed (7 chars, -1, remove d, HIGH phonetic match)\n"
                    "âœ“ Mohammad (8 chars, exact, swap e/a, HIGH phonetic match)\n"
                    "âœ“ Muhammed (9 chars, +1, double m, HIGH phonetic match)\n"
                    "âœ“ Mohamad (7 chars, -1, remove e, HIGH phonetic match)\n"
                    "âœ“ Muhamed (7 chars, -1, combine, GOOD phonetic match)\n"
                    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
                    "MANDATORY QUALITY CHECKLIST (Each Variation Must Pass ALL):\n"
                    "  âœ“ Sounds nearly identical when spoken aloud? (CRITICAL)\n"
                    "  âœ“ Looks visually similar (70%+ letter overlap)? (CRITICAL)\n"
                    "  âœ“ Within Â±2 characters of original length? (STRICT)\n"
                    "  âœ“ Same number of words (1â†’1, 2â†’2)? (REQUIRED)\n"
                    "  âœ“ Contains ONLY letters, spaces, hyphens, apostrophes? (REQUIRED)\n"
                    "  âœ“ NO numbers (0-9)? (FORBIDDEN)\n"
                    "  âœ“ NO address words (Street, Ave, etc.)? (FORBIDDEN)\n"
                    "  âœ“ NO date/month names? (FORBIDDEN)\n"
                    "  âœ“ Not identical to original? (REQUIRED)\n"
                    "  âœ“ Not a duplicate of another variation? (REQUIRED)\n"
                    "  âœ“ Realistic and plausible as a name? (REQUIRED)\n\n"
                    f"OUTPUT FORMAT (STRICT):\n"
                    f"Generate EXACTLY {self.target_variations} unique, high-quality variations.\n"
                    "Each variation must pass ALL checklist items above.\n"
                    "Preserve capitalization pattern from original (John â†’ Jon, not john).\n"
                    "Keep same word count as original name.\n"
                    "Return ONLY valid JSON with no preamble or explanation:\n"
                    "{\"variations\": [\"Variant1\", \"Variant2\", \"Variant3\", ...]}\n"
                    "NO markdown, NO code blocks, NO trailing text, JUST the JSON object."
                ),
            },
            {
                "role": "user",
                "content": (
                    f"{prompt}\n\n"
                    "REMINDER: Each variation MUST sound nearly IDENTICAL to the original when spoken aloud. "
                    "Test each variation by saying it out loud - if it doesn't sound like the original, reject it. "
                    "Prioritize phonetic accuracy over everything else."
                ),
            },
        ]

    def _get_cached_response(self, prompt: str) -> Optional[str]:
        """Retrieve a cached LLM response if available."""
        cached = self._response_cache.get(prompt)
        if cached is not None:
            self._response_cache.move_to_end(prompt)
        return cached

    def _cache_response(self, prompt: str, response: str) -> None:
        """Store the response while enforcing an upper bound on cache size."""
        self._response_cache[prompt] = response
        self._response_cache.move_to_end(prompt)
        while len(self._response_cache) > self.cache_max_entries:
            self._response_cache.popitem(last=False)

    def _normalize_and_capitalize(self, seed: str, variation: str) -> str:
        """Normalize spacing and match capitalization to the seed."""
        normalized = self._normalize_name(variation)
        return self._apply_capitalization_pattern(seed, normalized)
    
    def process_variations(self, Response_list: List[str], run_id: int, run_dir: str, identity_list: List[List[str]]) -> Dict[str, List[List[str]]]:
        """
        Process LLM responses to extract identity variations.

        This function takes the raw LLM responses and extracts the name variations
        using the Process_function. It then creates structured variations that include
        name, DOB, and address variations for each identity.

        Args:
            Response_list: List of LLM responses in the format:
                          ["Respond", "---", "Query-{name}", "---", "{LLM response}"]
            run_id: Unique identifier for this processing run
            run_dir: Directory to save run-specific files
            identity_list: List of identity arrays, each containing [name, dob, address]

        Returns:
            Dictionary mapping each name to its list of [name, dob, address] variations
        """
        bt.logging.info(f"Processing {len(Response_list)} responses")
        Responds = "".join(Response_list).split("Respond")
        name_variations: Dict[str, List[List[str]]] = {}

        for i in range(1, len(Responds)):
            try:
                llm_respond = self.Process_function(Responds[i], False)
                name = llm_respond[0]

                matching_identity = None
                seed_name_lower = name.lower()
                for identity in identity_list:
                    if len(identity) > 0 and identity[0] and identity[0].strip().lower() == seed_name_lower:
                        matching_identity = identity
                        break

                if matching_identity is None:
                    bt.logging.warning(f"Could not find identity for name {name}")
                    continue

                seed_address = matching_identity[2] if len(matching_identity) > 2 else "Unknown"
                seed_dob = matching_identity[1] if len(matching_identity) > 1 else "Unknown"

                variations = [
                    var for var in llm_respond[2]
                    if not pd.isna(var) and var != "" and var.strip()
                ]
                variations = self._deduplicate_variations(variations)
                
                # Filter out variations that are too different in length (for better length score)
                variations = self._filter_by_length(variations, name)
                
                # CRITICAL: Filter out low-quality variations (below 0.60 similarity score)
                # This ensures only high-scoring variations are kept, boosting overall scores
                variations = self._filter_by_quality(variations, name, min_quality=0.60)

                existing_variations = {seed_name_lower}
                existing_variations.update(var.lower() for var in variations)

                if len(variations) < self.max_variations:
                    needed = self.max_variations - len(variations)
                    heuristic_variations = self._generate_rule_based_variations(
                        name,
                        needed,
                        existing_variations,
                    )
                    if heuristic_variations:
                        bt.logging.debug(
                            f"Generated {len(heuristic_variations)} rule-based variations for {name} to reach target count"
                        )
                        variations.extend(heuristic_variations)

                if len(variations) > self.max_variations:
                    bt.logging.info(
                        f"Ranking {len(variations)} variations for {name} and selecting top {self.max_variations}"
                    )
                    variations = self._rank_and_filter_variations(variations, name, self.max_variations)
                elif variations:
                    # Even if we have fewer than max, still rank them for quality
                    variations = self._rank_and_filter_variations(variations, name, len(variations))

                structured_variations = []
                seen_variations = set()  # Track to ensure uniqueness
                seen_normalized = set()  # Track normalized forms to prevent collisions
                
                for var in variations:
                    cleaned_var = self._normalize_and_capitalize(name, var)

                    if cleaned_var and cleaned_var != name:  # Don't include exact match
                        # CRITICAL: Multi-level uniqueness checks to prevent penalties
                        cleaned_lower = cleaned_var.lower()
                        
                        # 1. Standard uniqueness (case-insensitive)
                        if cleaned_lower in seen_variations or cleaned_lower == name.lower():
                            continue
                        
                        # 2. Collision prevention (normalized form)
                        normalized_form = re.sub(r'[^a-z]', '', cleaned_lower)
                        if normalized_form in seen_normalized:
                            bt.logging.debug(f"Skipping potential collision: '{cleaned_var}' (normalized: {normalized_form})")
                            continue
                        
                        # 3. Duplication prevention (soundex similarity)
                        try:
                            var_soundex = jellyfish.soundex(cleaned_var)
                            is_duplicate = False
                            for existing_var in seen_variations:
                                try:
                                    if jellyfish.soundex(existing_var) == var_soundex and \
                                       jellyfish.levenshtein_distance(cleaned_lower, existing_var) <= 1:
                                        is_duplicate = True
                                        bt.logging.debug(f"Skipping near-duplicate: '{cleaned_var}' similar to '{existing_var}'")
                                        break
                                except:
                                    pass
                            if is_duplicate:
                                continue
                        except:
                            pass
                        
                        # All checks passed - add variation
                        seen_variations.add(cleaned_lower)
                        seen_normalized.add(normalized_form)
                        # Keep DOB and address consistent with seed (validators check this)
                        structured_variation = [cleaned_var, seed_dob, seed_address]
                        structured_variations.append(structured_variation)

                structured_variations = self._pad_structured_variations(
                    name, seed_dob, seed_address, structured_variations
                )

                # Ensure we have enough variations (completeness score)
                if len(structured_variations) == 0:
                    bt.logging.warning(f"No valid variations generated for {name}, creating fallback variations")
                    structured_variations = self._generate_fallback_variations(name, seed_dob, seed_address)

                structured_variations = self._pad_structured_variations(
                    name, seed_dob, seed_address, structured_variations
                )

                name_variations[name] = structured_variations
                bt.logging.info(f"Processed {len(structured_variations)} variations for {name}")
            except Exception as e:
                bt.logging.error(f"Error processing response {i}: {e}")
                bt.logging.error(f"Traceback: {traceback.format_exc()}")

        bt.logging.info(f"Generated structured variations: {name_variations}")
        return name_variations
    
    def _filter_by_quality(self, variations: List[str], original: str, min_quality: float = 0.65) -> List[str]:
        """
        CRITICAL: Filter variations to keep only high-quality ones above minimum similarity threshold.
        This maximizes phonetic and orthographic similarity scores (30% of total score).
        Raised threshold to 0.65 for maximum competitiveness.
        """
        if not variations:
            return []
        
        high_quality = []
        for var in variations:
            quality = self._score_variation_quality(var, original)
            if quality >= min_quality:
                high_quality.append(var)
            else:
                bt.logging.debug(f"Filtered out low-quality variation '{var}' (score: {quality:.3f} < {min_quality})")
        
        # If we filtered out everything, lower threshold slightly and try again
        if not high_quality and variations:
            bt.logging.warning(f"All variations below quality threshold {min_quality} for '{original}', trying 0.55")
            for var in variations:
                quality = self._score_variation_quality(var, original)
                if quality >= 0.55:
                    high_quality.append(var)
        
        # Last resort: return best ones anyway (ranked)
        if not high_quality and variations:
            bt.logging.warning(f"All variations below 0.55 threshold for '{original}', keeping best ranked")
            return self._rank_and_filter_variations(variations, original, min(len(variations), self.max_variations))
        
        return high_quality
    
    def _filter_by_length(self, variations: List[str], original: str) -> List[str]:
        """Filter variations to keep only those with similar length to original (Â±2 chars for optimal score)."""
        original_len = len(original)
        filtered = []
        for var in variations:
            # Allow Â±2 characters difference for optimal length score (strict constraint)
            if abs(len(var) - original_len) <= 2:
                filtered.append(var)
            else:
                bt.logging.debug(f"Filtered out '{var}' due to length difference from '{original}'")
        
        # If we have good matches, return them
        if filtered:
            return filtered

        # If nothing passed the strict filter, try Â±3 as fallback
        filtered_relaxed = []
        for var in variations:
            if abs(len(var) - original_len) <= 3:
                filtered_relaxed.append(var)
        
        if filtered_relaxed:
            return filtered_relaxed

        # Last resort: fall back to closest length matches
        ranked_by_length = sorted(
            variations,
            key=lambda candidate: abs(len(candidate) - original_len)
        )
        return ranked_by_length[: self.max_variations]
    
    def _score_variation_quality(self, variation: str, original: str) -> float:
        """
        Score a variation based on phonetic and orthographic similarity.
        Returns a score between 0.0 and 1.0 (higher is better).
        
        This scoring function evaluates variations across multiple dimensions:
        - Length similarity (prefer close to original length)
        - Character overlap (Jaccard similarity)
        - Edit distance (Levenshtein)
        - Phonetic similarity (Soundex and Metaphone)
        """
        try:
            original_lower = original.lower()
            variation_lower = variation.lower()
            
            # 1. Length similarity (closer to original is better)
            length_diff = abs(len(variation) - len(original))
            length_score = max(0, 1.0 - (length_diff / max(len(original), 1)))
            
            # 2. Character overlap (Jaccard similarity)
            original_chars = set(original_lower)
            variation_chars = set(variation_lower)
            if original_chars:
                jaccard = len(original_chars & variation_chars) / len(original_chars | variation_chars)
            else:
                jaccard = 0.0
            
            # 3. Levenshtein distance (normalized edit distance)
            lev_dist = Levenshtein.distance(original_lower, variation_lower)
            lev_score = max(0, 1.0 - (lev_dist / max(len(original), len(variation))))
            
            # 4. Phonetic similarity (Soundex) - CRITICAL for scoring
            try:
                soundex_orig = jellyfish.soundex(original)
                soundex_var = jellyfish.soundex(variation)
                # Perfect match = 1.0, no match = 0.0 (stricter)
                soundex_match = 1.0 if soundex_orig == soundex_var else 0.0
            except:
                soundex_match = 0.0
            
            # 5. Phonetic similarity (Metaphone) - CRITICAL for scoring
            try:
                metaphone_orig = jellyfish.metaphone(original)
                metaphone_var = jellyfish.metaphone(variation)
                # Perfect match = 1.0, no match = 0.0 (stricter)
                metaphone_match = 1.0 if metaphone_orig == metaphone_var else 0.0
            except:
                metaphone_match = 0.0
            
            # 6. Additional phonetic check using Jaro-Winkler for fine-grained similarity
            try:
                jaro_score = jellyfish.jaro_winkler_similarity(original_lower, variation_lower)
            except:
                jaro_score = 0.5
            
            # Additional structure alignment: token count and edge characters
            original_parts = original_lower.split()
            variation_parts = variation_lower.split()
            part_delta = abs(len(original_parts) - len(variation_parts))
            structure_score = 1.0 if part_delta == 0 else 0.75 if part_delta == 1 else 0.5

            edge_score = 0.0
            if variation_lower:
                if variation_lower[0] == original_lower[:1]:
                    edge_score += 0.5
                if variation_lower[-1:] == original_lower[-1:]:
                    edge_score += 0.5

            # Combined score (weighted average optimized for validator scoring)
            # Heavily weight phonetic similarity (45%) and orthographic similarity (40%)
            quality_score = (
                soundex_match * 0.18 +      # Phonetic (Soundex): 18%
                metaphone_match * 0.18 +    # Phonetic (Metaphone): 18%
                jaro_score * 0.12 +         # Jaro-Winkler similarity: 12%
                jaccard * 0.16 +            # Character overlap: 16%
                lev_score * 0.16 +          # Edit distance: 16%
                length_score * 0.12 +       # Length similarity: 12%
                structure_score * 0.05 +    # Word-count alignment: 5%
                edge_score * 0.03           # Preserve first/last character: 3%
            )
            
            return quality_score
            
        except Exception as e:
            bt.logging.debug(f"Error scoring variation quality: {e}")
            return 0.5  # Return neutral score on error
    
    def _rank_and_filter_variations(self, variations: List[str], original: str, max_count: int) -> List[str]:
        """
        Rank variations by quality score and return the top N.
        This ensures only the highest quality variations are sent to validators.
        """
        if not variations:
            return []
        
        # Score each variation
        scored_variations = []
        for var in variations:
            score = self._score_variation_quality(var, original)
            scored_variations.append((var, score))
        
        # Sort by score (highest first)
        scored_variations.sort(key=lambda x: x[1], reverse=True)
        
        # Log the quality distribution
        if scored_variations:
            avg_score = sum(s[1] for s in scored_variations) / len(scored_variations)
            top_score = scored_variations[0][1]
            bt.logging.debug(f"Variation quality for '{original}': avg={avg_score:.3f}, top={top_score:.3f}")
        
        # Return top N variations
        return [var for var, score in scored_variations[:max_count]]

    def _apply_capitalization_pattern(self, template: str, variation: str) -> str:
        """Match the capitalization pattern of the template name."""
        template_words = template.split()
        variation_words = variation.split()
        formatted_words: List[str] = []

        for idx, word in enumerate(variation_words):
            template_word = template_words[idx] if idx < len(template_words) else template_words[-1] if template_words else ""
            if template_word.isupper():
                formatted_words.append(word.upper())
            elif template_word[:1].isupper():
                formatted_words.append(word[:1].upper() + word[1:])
            else:
                formatted_words.append(word.lower())

        return " ".join(formatted_words)

    def _create_safe_variant(self, name: str, index: int) -> str:
        """Create a safe, high-quality phonetic variation that won't be flagged as address/DOB.
        
        CRITICAL: Uses phonetically optimized transformations indexed by iteration number.
        These variations score highly on phonetic similarity (30% of total score).
        """
        name_lower = name.lower()
        
        # OPTIMIZED: Phonetically-ranked transformations (high-scoring patterns first)
        transformations = [
            # Tier 1: IDENTICAL phonetic sound (highest score)
            lambda n: n.replace('ph', 'f', 1) if 'ph' in n else n,
            lambda n: n.replace('f', 'ph', 1) if 'f' in n and 'ph' not in n else n,
            lambda n: n.replace('c', 'k', 1) if 'c' in n and n.index('c') > 0 else n,
            lambda n: n.replace('k', 'c', 1) if 'k' in n else n,
            lambda n: n.replace('ck', 'k', 1) if 'ck' in n else n,
            
            # Tier 2: VERY SIMILAR phonetic sound
            lambda n: n.replace('s', 'z', 1) if 's' in n and n.index('s') > 0 else n,
            lambda n: n.replace('z', 's', 1) if 'z' in n else n,
            lambda n: n.replace('i', 'y', 1) if 'i' in n and n.index('i') > 0 else n,
            lambda n: n.replace('y', 'i', 1) if 'y' in n and n.index('y') > 0 else n,
            
            # Tier 3: Similar vowel sounds
            lambda n: n.replace('a', 'e', 1) if 'a' in n and n.index('a') > 0 else n,
            lambda n: n.replace('e', 'i', 1) if 'e' in n and n.index('e') > 0 else n,
            lambda n: n.replace('o', 'u', 1) if 'o' in n and n.index('o') > 0 else n,
            lambda n: n.replace('e', 'a', 1) if 'e' in n and n.index('e') > 0 else n,
            
            # Tier 4: Double letter variations (minimal phonetic change)
            lambda n: n.replace('tt', 't', 1) if 'tt' in n else (n.replace('t', 'tt', 1) if 't' in n and n.index('t') > 0 else n),
            lambda n: n.replace('ll', 'l', 1) if 'll' in n else (n.replace('l', 'll', 1) if 'l' in n and n.index('l') > 0 else n),
            lambda n: n.replace('ss', 's', 1) if 'ss' in n else (n.replace('s', 'ss', 1) if 's' in n and n.index('s') > 0 else n),
            lambda n: n.replace('nn', 'n', 1) if 'nn' in n else (n.replace('n', 'nn', 1) if 'n' in n and n.index('n') > 0 else n),
            
            # Tier 5: Silent letter tweaks (preserves phonetics)
            lambda n: n + 'e' if not n.endswith('e') and len(n) > 2 else (n[:-1] if n.endswith('e') and len(n) > 3 else n),
            lambda n: n.replace('gh', '', 1) if 'gh' in n else n,
            lambda n: n.replace('h', '', 1) if 'h' in n and n.index('h') > 0 and n.index('h') < len(n)-1 else n,
            
            # Tier 6: Additional consonant variations
            lambda n: n.replace('ch', 'k', 1) if 'ch' in n else n,
            lambda n: n.replace('qu', 'kw', 1) if 'qu' in n else n,
            lambda n: n.replace('x', 'ks', 1) if 'x' in n else n,
        ]
        
        transformation = transformations[index % len(transformations)]
        variant = transformation(name_lower)
        
        # Ensure variant is different from original
        if variant == name_lower and index < len(transformations):
            # Try next transformation
            transformation = transformations[(index + 1) % len(transformations)]
            variant = transformation(name_lower)
        
        # Apply capitalization from original
        return self._apply_capitalization_pattern(name, variant)
    
    def _generate_aggressive_variations(
        self,
        seed: str,
        count: int,
        existing_variations: Set[str],
    ) -> List[str]:
        """
        Generate more aggressive variations using multiple simultaneous transformations.
        Used when standard rule-based generation doesn't produce enough variations.
        """
        if count <= 0:
            return []
        
        variations: List[str] = []
        seed_lower = seed.lower()
        
        # Multi-transformation combinations
        transformations = [
            ('a', 'e'), ('e', 'i'), ('i', 'y'), ('o', 'u'), ('u', 'oo'),
            ('c', 'k'), ('k', 'c'), ('s', 'z'), ('z', 's'),
            ('ph', 'f'), ('f', 'ph'), ('ch', 'sh'),
            ('tt', 't'), ('ll', 'l'), ('ss', 's'),
        ]
        
        # Apply combinations of 2 transformations
        for i, trans1 in enumerate(transformations):
            if len(variations) >= count:
                break
                
            for trans2 in transformations[i+1:]:
                if len(variations) >= count:
                    break
                
                variant = seed_lower
                # Apply first transformation
                if trans1[0] in variant:
                    variant = variant.replace(trans1[0], trans1[1], 1)
                # Apply second transformation
                if trans2[0] in variant:
                    variant = variant.replace(trans2[0], trans2[1], 1)
                
                # Validate length and uniqueness
                if abs(len(variant) - len(seed)) <= 3:
                    normalized_lower = variant.lower()
                    if normalized_lower not in existing_variations:
                        formatted = self._apply_capitalization_pattern(seed, variant)
                        # Validate it doesn't look like address/DOB
                        if not self._looks_like_address(formatted) and not self._looks_like_dob(formatted):
                            variations.append(formatted)
                            existing_variations.add(normalized_lower)
        
        return variations
    
    def _generate_rule_based_variations(
        self,
        seed: str,
        count: int,
        existing_variations: Set[str],
    ) -> List[str]:
        """
        Generate additional variations using deterministic phonetic/orthographic rules.
        This is used to top-up the variation list when the LLM does not return enough items.
        """
        if count <= 0:
            return []

        variations: List[str] = []
        seed_lower = seed.lower()

        # Phonetically optimized vowel swaps (prioritize sound-alike)
        vowel_swaps = {
            "a": ["e"],           # alan â†’ elen (subtle)
            "e": ["i", "a"],      # debra â†’ dibra, debra â†’ dabra
            "i": ["y", "e"],      # smith â†’ smyth, smith â†’ smeth
            "o": ["u", "a"],      # jon â†’ jun, jon â†’ jan
            "u": ["o"],           # but â†’ bot
            "y": ["i"],           # smyth â†’ smith
        }

        # Phonetically optimized consonant patterns (prioritize identical sound)
        pattern_replacements = {
            "ph": ["f"],          # philip â†’ filip (IDENTICAL sound)
            "f": ["ph"],          # reverse
            "ck": ["k"],          # rick â†’ rik (IDENTICAL sound)
            "k": ["c"],           # karl â†’ carl (IDENTICAL sound)
            "c": ["k"],           # reverse
            "s": ["z"],           # susan â†’ zusan (similar sound)
            "z": ["s"],           # reverse
            "ch": ["k"],          # christopher â†’ kristopher (similar)
            "x": ["ks"],          # alex â†’ aleks
            "qu": ["kw"],         # quincy â†’ kwincy
            "ie": ["y", "i"],     # ie variations
            "gh": [""],           # knight â†’ knit (silent gh)
        }

        double_letters = ["tt", "ll", "ss", "nn", "rr", "mm"]
        accent_map = {
            "a": ["Ã¡", "Ã ", "Ã¢"],
            "e": ["Ã©", "Ã¨", "Ãª"],
            "i": ["Ã­", "Ã¬"],
            "o": ["Ã³", "Ã²", "Ã´"],
            "u": ["Ãº", "Ã¹"],
        }

        def add_variant(candidate: str) -> None:
            normalized = self._normalize_name(candidate)
            if not normalized:
                return

            normalized_lower = normalized.lower()
            if normalized_lower in existing_variations:
                return

            if abs(len(normalized) - len(seed)) > 3:
                return
            
            # CRITICAL: Validate no address/DOB patterns
            if self._looks_like_address(normalized) or self._looks_like_dob(normalized):
                return

            formatted = self._normalize_and_capitalize(seed, normalized)
            existing_variations.add(normalized_lower)
            variations.append(formatted)

        # 1. Vowel swaps (avoid first and last character for better edge preservation)
        for idx, char in enumerate(seed_lower):
            # Skip first and last character to preserve edges
            if idx == 0 or idx == len(seed_lower) - 1:
                continue
            
            replacements = vowel_swaps.get(char, [])
            for repl in replacements:
                new_variant = seed_lower[:idx] + repl + seed_lower[idx + 1 :]
                add_variant(new_variant)
                if len(variations) >= count:
                    return variations

        # 2. Common pattern replacements
        for pattern, replacements in pattern_replacements.items():
            if pattern in seed_lower:
                for repl in replacements:
                    new_variant = seed_lower.replace(pattern, repl, 1)
                    add_variant(new_variant)
                    if len(variations) >= count:
                        return variations

        # 3. Double letter manipulations
        for dbl in double_letters:
            if dbl in seed_lower:
                new_variant = seed_lower.replace(dbl, dbl[0], 1)
                add_variant(new_variant)
                if len(variations) >= count:
                    return variations
            elif dbl[0] in seed_lower:
                new_variant = seed_lower.replace(dbl[0], dbl, 1)
                add_variant(new_variant)
                if len(variations) >= count:
                    return variations

        # 4. Accent/diacritic variations
        for idx, char in enumerate(seed_lower):
            replacements = accent_map.get(char, [])
            for repl in replacements:
                new_variant = seed_lower[:idx] + repl + seed_lower[idx + 1 :]
                add_variant(new_variant)
                if len(variations) >= count:
                    return variations

        # 5. Silent letter tweaks
        if "h" in seed_lower:
            new_variant = seed_lower.replace("h", "", 1)
            add_variant(new_variant)
        if not seed_lower.endswith("e"):
            add_variant(seed_lower + "e")
        else:
            add_variant(seed_lower[:-1])

        # 6. Swap first/last characters for short names
        if len(seed_lower.replace(" ", "")) >= 4:
            words = seed_lower.split()
            primary_word = words[0]
            if len(primary_word) > 3:
                swapped = primary_word[-1] + primary_word[1:-1] + primary_word[0]
                new_variant = " ".join([swapped] + words[1:])
                add_variant(new_variant)

        if len(variations) > count:
            ranked = self._rank_and_filter_variations(variations, seed, count)
            return ranked

        return variations
    
    def _generate_fallback_variations(self, name: str, dob: str, address: str, count: int = 5) -> List[List[str]]:
        """Generate deterministic fallback variations when the LLM fails.
        
        CRITICAL: This ensures 100% completeness by always generating variations for every name.
        """
        existing_variations = {name.lower()}
        target_count = max(count, self.max_variations)
        
        # Use aggressive rule-based generation for fallback
        fallback_names = self._generate_rule_based_variations(
            name,
            target_count,
            existing_variations,
        )

        # If still not enough, add more aggressive variations
        if len(fallback_names) < target_count:
            # Generate additional variations using more permutations
            additional = self._generate_aggressive_variations(
                name,
                target_count - len(fallback_names),
                existing_variations | {v.lower() for v in fallback_names}
            )
            fallback_names.extend(additional)

        # Ensure we have at least the target count
        if not fallback_names:
            fallback_names = [self._apply_capitalization_pattern(name, name)]

        structured: List[List[str]] = []
        for variant in fallback_names:
            # Validate before adding
            if not self._looks_like_address(variant) and not self._looks_like_dob(variant):
                structured.append([variant, dob, address])
                if len(structured) >= target_count:
                    break

        # CRITICAL: If we still don't have enough, pad with safe variations
        while len(structured) < min(target_count, self.max_variations):
            bt.logging.warning(f"Padding variations for {name} to meet minimum count")
            # Create safe variations by cycling through vowel changes
            base_variant = self._create_safe_variant(name, len(structured))
            if base_variant and base_variant.lower() not in existing_variations:
                structured.append([base_variant, dob, address])
                existing_variations.add(base_variant.lower())

        return structured

    def _pad_structured_variations(
        self,
        seed_name: str,
        seed_dob: str,
        seed_address: str,
        structured_variations: List[List[str]],
    ) -> List[List[str]]:
        """Top up structured variations with deterministic rule-based candidates for better count/uniqueness scores."""
        target_count = min(self.target_variations, self.max_variations)

        if len(structured_variations) >= target_count:
            return structured_variations[:target_count]

        existing_variations = {seed_name.lower()}
        existing_variations.update(variation[0].lower() for variation in structured_variations)

        needed = target_count - len(structured_variations)
        
        # First try standard rule-based variations
        additional_names = self._generate_rule_based_variations(
            seed_name,
            needed,
            existing_variations,
        )

        for variant in additional_names:
            if len(structured_variations) >= target_count:
                break
            structured_variations.append([variant, seed_dob, seed_address])
            existing_variations.add(variant.lower())
        
        # If still need more, use aggressive variations
        if len(structured_variations) < target_count:
            still_needed = target_count - len(structured_variations)
            aggressive_names = self._generate_aggressive_variations(
                seed_name,
                still_needed,
                existing_variations,
            )
            
            for variant in aggressive_names:
                if len(structured_variations) >= target_count:
                    break
                structured_variations.append([variant, seed_dob, seed_address])
                existing_variations.add(variant.lower())
        
        # Final padding with safe variants if still needed
        iteration = 0
        while len(structured_variations) < target_count and iteration < 50:
            safe_variant = self._create_safe_variant(seed_name, iteration)
            if safe_variant and safe_variant.lower() not in existing_variations:
                # Validate no address/DOB patterns
                if not self._looks_like_address(safe_variant) and not self._looks_like_dob(safe_variant):
                    structured_variations.append([safe_variant, seed_dob, seed_address])
                    existing_variations.add(safe_variant.lower())
            iteration += 1

        return structured_variations
    
    def save_variations_to_json(self, name_variations: Dict[str, List[str]], run_id: int, run_dir: str) -> None:
        """
        Save processed variations to JSON and DataFrame for debugging and analysis.
        
        This function saves the processed variations in multiple formats:
        1. A pandas DataFrame saved as a pickle file in the run-specific directory
        2. A JSON file with the name variations in the run-specific directory
        3. A JSON file with the model name and run ID in the main output directory
        
        Each file is named with the run ID to distinguish between different runs.
        
        Args:
            name_variations: Dictionary mapping names to variations
            run_id: Unique identifier for this processing run
            run_dir: Directory to save run-specific files
        """
        bt.logging.info(f"=================== Name variations: {name_variations}")
        bt.logging.info(f"=================== Run ID: {run_id}")
        bt.logging.info(f"=================== Run directory: {run_dir}")
        bt.logging.info("Saving variations to JSON and DataFrame")

        # Find the maximum number of variations for any name
        max_variations = max([len(vars) for vars in name_variations.values()]) if name_variations else 0
        bt.logging.info(f"Maximum number of variations found: {max_variations}")
        
        # Create a DataFrame with columns for the name and each variation
        columns = ['Name'] + [f'Var_{i+1}' for i in range(max_variations)]
        result_df = pd.DataFrame(columns=columns)
        
        # Fill the DataFrame with names and their variations, padding with empty strings if needed
        for i, (name, variations) in enumerate(name_variations.items()):
            row_data = [name] + variations + [''] * (max_variations - len(variations))
            result_df.loc[i] = row_data
        
        # Note: We no longer need to clean the data here since it's already cleaned
        # in the process_variations function
        
        # Save DataFrame to pickle for backup and analysis
        # Include run_id in the filename
        #df_path = os.path.join(run_dir, f"variations_df_{run_id}.pkl")
        #result_df.to_pickle(df_path)
        
        # Convert DataFrame to JSON format
        json_data = {}
        for i, row in result_df.iterrows():
            name = row['Name']
            # Extract non-empty variations
            variations = [var for var in row[1:] if var != ""]
            json_data[name] = variations

        # Save to JSON file
        # Include run_id in the filename
        # json_path = os.path.join(run_dir, f"variations_{run_id}.json")
        # import json
        # with open(json_path, 'w', encoding='utf-8') as f:
        #     json.dump(json_data, f, indent=4)
        # bt.logging.info(f"Saved variations to: {json_path}")
        # bt.logging.info(f"DataFrame shape: {result_df.shape} with {max_variations} variation columns")

    def _deduplicate_variations(self, variations: List[str]) -> List[str]:
        """Return a list of unique variations while preserving order."""
        seen = set()
        deduped = []

        for variation in variations:
            normalized = variation.lower()
            if normalized in seen:
                continue

            seen.add(normalized)
            deduped.append(variation)

        return deduped
    
    def Clean_extra(self, payload: str, comma: bool, line: bool, space: bool, preserve_name_spaces: bool = False) -> str:
        """
        Clean the LLM output by removing unwanted characters.
        
        Args:
            payload: The text to clean
            comma: Whether to remove commas
            line: Whether to remove newlines
            space: Whether to remove spaces
            preserve_name_spaces: Whether to preserve spaces between names (for multi-part names)
        """
        # Remove punctuation and quotes
        payload = payload.replace(".", "")
        payload = payload.replace('"', "")
        payload = payload.replace("'", "")
        payload = payload.replace("-", "")
        payload = payload.replace("and ", "")
        
        # Handle spaces based on preservation flag
        if space:
            if preserve_name_spaces:
                # Replace multiple spaces with single space
                while "  " in payload:
                    payload = payload.replace("  ", " ")
            else:
                # Original behavior - remove all spaces
                payload = payload.replace(" ", "")
        
        if comma:
            payload = payload.replace(",", "")
        if line:
            payload = payload.replace("\\n", "")
        
        return payload.strip()

    def validate_variation(self, name: str, seed: str, is_multipart_name: bool) -> str:
        """
        Helper function to validate if a variation matches the seed name structure.
        
        Args:
            name: The variation to validate
            seed: The original seed name
            is_multipart_name: Whether the seed is a multi-part name
            
        Returns:
            str: The validated and cleaned variation, or np.nan if invalid
        """
        name = self._normalize_name(name)
        if not name or name.isspace():
            return np.nan

        # Remove common prefixes from LLM responses
        for prefix in ["Variation:", "Alt:", "Alternative:", "-", "*", "â€¢"]:
            if name.startswith(prefix):
                name = name[len(prefix):].strip()
        
        # CRITICAL: Reject variations that look like addresses (numbers, street indicators)
        # This prevents the catastrophic "Looks Like Address" penalty
        if self._looks_like_address(name):
            bt.logging.debug(f"Rejecting variation '{name}' - looks like an address")
            return np.nan
        
        # CRITICAL: Reject variations that look like dates or DOBs
        if self._looks_like_dob(name):
            bt.logging.debug(f"Rejecting variation '{name}' - looks like a date/DOB")
            return np.nan
        
        # STRICT: Reject names containing unexpected symbols to avoid penalties
        # Only allow: letters, spaces, hyphens, apostrophes, and basic accented characters
        if not re.match(r"^[A-Za-zÃ€-Ã¿]+([ '\-][A-Za-zÃ€-Ã¿]+)*$", name):
            bt.logging.debug(f"Skipping variation '{name}' due to unexpected characters")
            return np.nan
        
        # CRITICAL: Reject if variation contains common problematic patterns
        if self._contains_problematic_patterns(name):
            bt.logging.debug(f"Rejecting variation '{name}' - contains problematic patterns")
            return np.nan
        
        # Handle cases with colons (e.g., "Here are variations: Name")
        if ":" in name:
            name = name.split(":")[-1].strip()
        
        # Remove numbering (e.g., "1. Name" or "1) Name")
        name = re.sub(r'^\d+[\.\)]\s*', '', name)
        
        # Skip if empty after cleaning
        if not name or name.isspace():
            return np.nan
        
        # Skip if it's just the seed name (exact match)
        if name.lower() == seed.lower():
            return np.nan
        
        # Check length reasonability - strict (Â±3 characters max)
        # This optimizes length score
        if abs(len(name) - len(seed)) > 3:
            bt.logging.debug(f"Skipping variation '{name}' - length difference too large from '{seed}'")
            return np.nan
        
        # Check structure consistency with seed name (but be more lenient)
        name_parts = name.split()
        seed_parts = seed.split()
        
        if is_multipart_name:
            # For multi-part seed names (e.g., "John Smith"), variations should also have multiple parts
            # But allow some flexibility (e.g., 2-part seed can have 1-3 part variations)
            if len(name_parts) < 1:
                return np.nan
            # Be more lenient - allow variations with different number of parts if close
            if abs(len(name_parts) - len(seed_parts)) > 1:
                bt.logging.debug(f"Skipping variation '{name}' - part count too different from '{seed}'")
                return np.nan
        else:
            # For single-part seed names, prefer single-part variations but allow 2-part
            if len(name_parts) > 2:
                bt.logging.debug(f"Skipping multi-part variation '{name}' for single-part seed '{seed}'")
                return np.nan
            
        return name

    def Process_function(self, string: str, debug: bool) -> Tuple[str, str, List[str], Optional[str]]:
        """
        Process the LLM response to extract the seed name and variations.
        
        This function parses the LLM response to extract:
        1. The original seed name
        2. The list of name variations
        
        It handles different response formats from LLMs:
        - Comma-separated lists (preferred format)
        - Line-separated lists
        - Space-separated lists with numbering
        
        The function ensures variations match the structure of the seed name:
        - Single-part seed names (e.g., "John") only get single-part variations
        - Multi-part seed names (e.g., "John Smith") only get multi-part variations
        
        Args:
            string: The LLM response in the format:
                   "---\nQuery-{name}\n---\n{response}"
            debug: Whether to return debug information
            
        Returns:
            Tuple containing:
            - seed_name: The original name
            - processing_method: The method used to process the response (r1, r2, or r3)
            - variations_list: The list of extracted variations
            - payload: (if debug=True) The processed payload
        """
        # Split the response by "---" to extract the query and response parts
        splits = string.split('---')
        if len(splits) < 3 or "-" not in splits[1]:
            raise ValueError("Unexpected response format from LLM")

        # Extract and analyze the seed name structure
        seed = splits[1].split("-")[1].replace(".", "").replace(",", "").replace("'", "")
        seed_parts = seed.split()
        is_multipart_name = len(seed_parts) > 1
        seed = self.Clean_extra(seed, True, True, True, preserve_name_spaces=is_multipart_name)
        
        bt.logging.info(f"Processing seed name: '{seed}' (multipart: {is_multipart_name})")
        
        # Extract the response payload
        payload = splits[-1]

        json_result = self._try_parse_json_variations(payload, seed, is_multipart_name)
        if json_result is not None:
            parsed_seed, method, variations = json_result
            if debug:
                return parsed_seed, method, variations, payload
            return parsed_seed, method, variations
        
        # Case 1: Comma-separated list (preferred format)
        if len(payload.split(",")) > 3:  # Check if we have at least 3 commas
            # Clean the payload but keep commas for splitting
            payload = self.Clean_extra(payload, False, True, True, preserve_name_spaces=is_multipart_name)
            
            # Remove numbering prefixes
            for num in range(10):
                payload = payload.replace(str(num), "")
            
            # Split by comma and process each variation
            variations = []
            for name in payload.split(","):
                cleaned_var = self.validate_variation(name, seed, is_multipart_name)
                if not pd.isna(cleaned_var):
                    variations.append(cleaned_var)
            
            if debug:
                return seed, "r1", variations, payload
            return seed, "r1", variations
        
        # Case 2 & 3: Non-comma separated formats
        else:
            # Case 2: Line-separated list
            len_ans = len(payload.split("\\n"))
            if len_ans > 2:  # Multiple lines indicate line-separated format
                # Clean the payload but preserve newlines for splitting
                payload = self.Clean_extra(payload, True, False, True, preserve_name_spaces=is_multipart_name)
                
                # Remove numbering prefixes
                for num in range(10):
                    payload = payload.replace(str(num), "")
                
                # Process line-separated variations
                variations = []
                for name in payload.split("\\n"):
                    cleaned_var = self.validate_variation(name, seed, is_multipart_name)
                    if not pd.isna(cleaned_var):
                        variations.append(cleaned_var)
            
                if debug:
                    return seed, "r2", variations, payload
                return seed, "r2", variations
            
            # Case 3: Space-separated list
            else:
                # Clean the payload but preserve spaces for multi-part names
                payload = self.Clean_extra(payload, True, True, False, preserve_name_spaces=is_multipart_name)
                
                # Remove numbering prefixes
                for num in range(10):
                    payload = payload.replace(str(num), "")
                
                variations = []
                if is_multipart_name:
                    # For multi-part names, we need to carefully group the parts
                    current_variation = []
                    parts = payload.split()
                    
                    for part in parts:
                        part = part.strip()
                        if not part:
                            continue
                        
                        if ":" in part:  # New variation starts after colon
                            if current_variation:
                                # Process completed variation
                                cleaned_var = self.validate_variation(" ".join(current_variation), seed, is_multipart_name)
                                if not pd.isna(cleaned_var):
                                    variations.append(cleaned_var)
                            current_variation = [part.split(":")[-1].strip()]
                        else:
                            current_variation.append(part)
                            # Check if we have collected enough parts for a complete name
                            if len(current_variation) == len(seed_parts):
                                cleaned_var = self.validate_variation(" ".join(current_variation), seed, is_multipart_name)
                                if not pd.isna(cleaned_var):
                                    variations.append(cleaned_var)
                                current_variation = []
                
                    # Handle any remaining parts
                    if current_variation:
                        cleaned_var = self.validate_variation(" ".join(current_variation), seed, is_multipart_name)
                        if not pd.isna(cleaned_var):
                            variations.append(cleaned_var)
                else:
                    # For single-part names, simple space splitting is sufficient
                    for name in payload.split():
                        cleaned_var = self.validate_variation(name, seed, is_multipart_name)
                        if not pd.isna(cleaned_var):
                            variations.append(cleaned_var)
                
            if debug:
                return seed, "r3", variations, payload
            return seed, "r3", variations

    def _try_parse_json_variations(
        self, payload: str, seed: str, is_multipart_name: bool
    ) -> Optional[Tuple[str, str, List[str]]]:
        """Quickly parse structured JSON responses for lower latency processing."""
        try:
            cleaned_payload = payload.strip()
            
            # Try multiple extraction strategies
            # Strategy 1: Extract from markdown code blocks
            if "```" in cleaned_payload:
                # Try json code block first
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', cleaned_payload, re.DOTALL)
                if json_match:
                    cleaned_payload = json_match.group(1)
                else:
                    # Try plain code block
                    code_match = re.search(r'```\s*(\{.*?\})\s*```', cleaned_payload, re.DOTALL)
                    if code_match:
                        cleaned_payload = code_match.group(1)
            
            # Strategy 2: Find JSON object directly
            if not cleaned_payload.strip().startswith('{'):
                json_obj_match = re.search(r'\{[^{}]*"variations"[^{}]*\[[^\]]*\][^{}]*\}', cleaned_payload, re.DOTALL)
                if json_obj_match:
                    cleaned_payload = json_obj_match.group(0)

            data = json.loads(cleaned_payload)
            variations_field = data.get("variations") if isinstance(data, dict) else None
            if not isinstance(variations_field, list):
                bt.logging.debug("No 'variations' list found in JSON response")
                return None

            variations: List[str] = []
            for entry in variations_field:
                # Handle both string entries and dict entries
                if isinstance(entry, dict):
                    # Try common keys that might contain the variation
                    variation_str = entry.get('name') or entry.get('variation') or entry.get('value') or str(entry)
                else:
                    variation_str = str(entry)
                
                cleaned_var = self.validate_variation(variation_str, seed, is_multipart_name)
                if not pd.isna(cleaned_var) and cleaned_var.strip():
                    variations.append(cleaned_var)

            if variations:
                bt.logging.debug(f"Successfully extracted {len(variations)} variations from JSON")
                return seed, "json", variations
            else:
                bt.logging.debug("No valid variations found in JSON response")
                return None
                
        except json.JSONDecodeError as exc:
            bt.logging.trace(f"JSON parsing failed: {exc}")
            return None
        except Exception as exc:
            bt.logging.trace(f"JSON parsing error, falling back to heuristic parser: {exc}")
            return None

    async def blacklist(
        self, synapse: IdentitySynapse
    ) -> typing.Tuple[bool, str]:
        """
        Determines whether an incoming request should be blacklisted and thus ignored.
        
        This function implements security checks to ensure that only authorized
        validators can query this miner. It verifies:
        1. Whether the request has a valid dendrite and hotkey
        2. Whether the hotkey is one of the ones on the white list
        
        Args:
            synapse: A IdentitySynapse object constructed from the incoming request.

        Returns:
            Tuple[bool, str]: A tuple containing:
                - bool: Whether the request should be blacklisted
                - str: The reason for the decision
        """
        # Check if the request has a valid dendrite and hotkey
        if synapse.dendrite is None or synapse.dendrite.hotkey is None:
            bt.logging.warning(
                "Received a request without a dendrite or hotkey."
            )
            return True, "Missing dendrite or hotkey"

        if synapse.dendrite.hotkey not in self.WHITELISTED_VALIDATORS:
            bt.logging.trace(
                f"Blacklisting un-registered hotkey {synapse.dendrite.hotkey}"
            )
            return True, "Unrecognized hotkey"

        # If all checks pass, allow the request
        bt.logging.trace(
            f"Not Blacklisting recognized hotkey {synapse.dendrite.hotkey}"
        )
        return False, "Hotkey recognized!"

    async def priority(self, synapse: IdentitySynapse) -> float:
        """
        The priority function determines the order in which requests are handled.
        
        This function assigns a priority to each request based on the stake of the
        calling entity. Requests with higher priority are processed first, which
        ensures that validators with more stake get faster responses.
        
        Args:
            synapse: The IdentitySynapse object that contains metadata about the incoming request.

        Returns:
            float: A priority score derived from the stake of the calling entity.
                  Higher values indicate higher priority.
        """
        # Check if the request has a valid dendrite and hotkey
        if synapse.dendrite is None or synapse.dendrite.hotkey is None:
            bt.logging.warning(
                "Received a request without a dendrite or hotkey."
            )
            return 0.0

        # Get the UID of the caller
        caller_uid = self.metagraph.hotkeys.index(
            synapse.dendrite.hotkey
        )
        
        # Use the stake as the priority
        # Higher stake = higher priority
        priority = float(
            self.metagraph.S[caller_uid]
        )
        
        bt.logging.trace(
            f"Prioritizing {synapse.dendrite.hotkey} with value: {priority}"
        )
        return priority


# This is the main function, which runs the miner.
if __name__ == "__main__":
    with Miner() as miner:
        while True:
            bt.logging.info(f"----------------------------------Name Variation Miner running... {time.time()}")
            time.sleep(30)
